{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UD8jYvDpAYM",
        "outputId": "5a1b5a71-a9a2-417e-b66b-7d53258a9bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=a0d3de30545a5ab2acbe868a9d6f72f21fd2dbaba09009a9f097946bcf4e8bce\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part -2\n",
        "#===============\n",
        "# Task -1\n",
        "#===============\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, min, max\n",
        "from pyspark.sql.functions import avg\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AirBnB Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "def get_data(url = 'https://github.com/databricks/LearningSparkV2/blob/master/mlflow-project-example/data/sf-airbnb-clean.parquet/part-00000-tid-4320459746949313749-5c3d407c-c844-4016-97ad-2edec446aa62-6688-1-c000.snappy.parquet?raw=true'):\n",
        "\n",
        "    \"\"\"  \n",
        "    This function loads the parquet file to Spark Dataframe API.\n",
        "    Args: \n",
        "      url: input parquet file path mapped from github \n",
        "    Returns:\n",
        "      This function returns Spark Dataframe API\n",
        "    \"\"\"\n",
        "    # download the parquet file \n",
        "    response = requests.get(url)\n",
        "    open('sf-airbnb-clean.parquet', 'wb').write(response.content)\n",
        "    import pandas as pd\n",
        "    # read the parquet file into a pandas dataframe\n",
        "    df = pd.read_parquet('sf-airbnb-clean.parquet')\n",
        "    # save the dataframe as a CSV file\n",
        "    df.to_csv('sf-airbnb-clean.csv', index=False)\n",
        "\n",
        "    Spark_df = spark.read.csv(\"sf-airbnb-clean.csv\", header=True, inferSchema=True)\n",
        "    return Spark_df\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "plARUTVnpxFE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=================================================================================\n",
        "#Task - 2 \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, min, max\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AirBnB Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def write_out_2_2(write_path='out/out_2_2.txt'):\n",
        "\n",
        "  \"\"\"  \n",
        "  This function writes the output by executing functions agg_df.\n",
        "  Args: \n",
        "    write_path: output file path for Part 2 task 2\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  agg_df_val = agg_df()\n",
        "  # write to a CSV file\n",
        "  agg_df_val.write.csv(write_path, header=True)\n",
        "\n",
        "def agg_df():\n",
        "\n",
        "  \"\"\"  \n",
        "  This function takes the input by excuting get_data and returns the minimum price, maximum price, and total row count.\n",
        "  Args: \n",
        "    None \n",
        "  Returns:\n",
        "    This functions returns aggrigated values such as minimum price, maximum price, and total row count \n",
        "  \"\"\"\n",
        "\n",
        "  Spark_df = get_data()\n",
        "# compute aggregates and store in a new DataFrame\n",
        "  agg_df_val = Spark_df.agg(min(\"price\").alias(\"min_price\"), \n",
        "                  max(\"price\").alias(\"max_price\"), \n",
        "                  count(\"*\").alias(\"row_count\"))\n",
        "  return agg_df_val \n",
        "\n",
        "write_out_2_2()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "zU3IjbPkVP2H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#====================================================================\n",
        "# task 3\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, min, max\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AirBnB Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def write_out_2_3(write_path='out/out_2_3.txt'):\n",
        "\n",
        "  \"\"\"  \n",
        "  This function writes the output out_2_3.txt by executing functions avg_df.\n",
        "  Args: \n",
        "    write_path: output file path for Part 2 task 3\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  avg_df_val = avg_df()\n",
        "\n",
        "    # write to a CSV file\n",
        "  avg_df_val.write.csv(write_path, header=True)\n",
        "\n",
        "def avg_df():\n",
        "\n",
        "  \"\"\"  \n",
        "  This function takes the input by excuting get_data\n",
        "  Args: \n",
        "    None \n",
        "  Returns:\n",
        "    This functions returns average number \n",
        "    of bathrooms and bedrooms across all the properties listed in this data set with a price of > 5000 \n",
        "    and a review score being exactly equalt to 10. \n",
        "  \"\"\"\n",
        "\n",
        "  Spark_df = get_data()\n",
        "  # filter the relevant rows and compute averages\n",
        "  avg_df_val = Spark_df.filter((Spark_df.price > 5000) & (Spark_df.review_scores_rating == 10)) \\\n",
        "            .agg(avg(\"bathrooms\").alias(\"avg_bathrooms\"), \n",
        "                  avg(\"bedrooms\").alias(\"avg_bedrooms\"))\n",
        "  return avg_df_val\n",
        "\n",
        "write_out_2_3()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "2Gvc2SRkKD3j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task_4\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, min, max\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AirBnB Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def write_out_2_4(write_path='out/out_2_4.txt'):\n",
        " \n",
        "  \"\"\"  \n",
        "  This function writes the output out_2_4.txt by executing functions capacity.\n",
        "  Args: \n",
        "    write_path: output file path for Part 2 task 4\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  capacity_val = capacity()\n",
        "    # write to a text file\n",
        "  with open(write_path, \"w\") as f:\n",
        "      f.write(str(capacity_val))\n",
        "\n",
        "def capacity():\n",
        "\n",
        "  \"\"\"  \n",
        "  This function takes the input by excuting get_data\n",
        "  Args: \n",
        "    None \n",
        "  Returns:\n",
        "    This functions returns How many people can be accomodated by the property with the lowest price and highest rating. \n",
        "  \"\"\"\n",
        "\n",
        "  Spark_df = get_data()\n",
        "  # sort the DataFrame by price and rating and select the first row\n",
        "  top_property = Spark_df.orderBy([\"price\", \"review_scores_rating\"], ascending=[True, False]).first()\n",
        "  # compute the maximum capacity\n",
        "  capacity_val = top_property.bedrooms * top_property.beds\n",
        "  return capacity_val\n",
        "\n",
        "write_out_2_4()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "o8AB7wz0LZNv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCdPaustAlRJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvPx7E58AlbY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Zk7LX4fAljI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAuGBZyqAlqw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eyn4mmUcAlyU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61F6GwS2Al2X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lD5hmm1GAl5M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmut8iaKAl8l"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}