{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fBqlcrgpJ51",
        "outputId": "692a704d-1f9f-4af2-d971-3a97ea58c2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=22baab5c4461f0f01003589f869058ec2490d80469bf1634bce81ac5668e2402\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "\n",
        "spark = SparkSession.builder.appName(\"LogisticRegressionExample\").getOrCreate()\n",
        "\n",
        "def get_irish_data(url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"):\n",
        "\n",
        "  \"\"\"  \n",
        "  This function loads the irish data to Spark Dataframe API.\n",
        "  Args: \n",
        "    url: input file path mapped from Iris Dataset UCI Machine Learning Repository.\n",
        "  Returns:\n",
        "    This function returns spark Dataframe API variable.\n",
        "  \"\"\"\n",
        "  \n",
        "  # File path to save the dataset\n",
        "  file_path = \"/tmp/iris.csv\"\n",
        "\n",
        "  # Download the file using urllib\n",
        "  urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "  # Load the dataset into a pandas dataframe\n",
        "  df = pd.read_csv(file_path, header=None, names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
        "  df.to_csv('iris.csv', index=False)\n",
        "  # Display the first few rows of the dataframe\n",
        "  # print(df)\n",
        "  # Create a SparkSession object\n",
        "  spark = SparkSession.builder.appName(\"LogisticRegressionExample\").getOrCreate()\n",
        "\n",
        "  # Load the data into a Spark DataFrame.\n",
        "  data = spark.read.csv(\"/content/iris.csv\", header=False, inferSchema=True)\n",
        "  data = data.select(col(\"_c0\").alias(\"sepal_length\"),\n",
        "                col(\"_c1\").alias(\"sepal_width\"),\n",
        "                col(\"_c2\").alias(\"petal_length\"),\n",
        "                col(\"_c3\").alias(\"petal_width\"),\n",
        "                col(\"_c4\").alias(\"class\"))\n",
        "  return data\n",
        "\n"
      ],
      "metadata": {
        "id": "KjuD_YXHDnJg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_model_build():\n",
        "\n",
        "  \"\"\"  \n",
        "  This function takes the input by excuting get_irish_data and builds a logistic regression machine learning model\n",
        "  Args: \n",
        "    None\n",
        "  Returns:\n",
        "    This function returns machine learning model and the test data for evaluation.\n",
        "  \"\"\"\n",
        "\n",
        "  data = get_irish_data()\n",
        "\n",
        "  # Cast the columns to double data type\n",
        "  data = data.withColumn(\"sepal_length\", col(\"sepal_length\").cast(\"double\"))\n",
        "  data = data.withColumn(\"sepal_width\", col(\"sepal_width\").cast(\"double\"))\n",
        "  data = data.withColumn(\"petal_length\", col(\"petal_length\").cast(\"double\"))\n",
        "  data = data.withColumn(\"petal_width\", col(\"petal_width\").cast(\"double\"))\n",
        "\n",
        "  # Convert class labels to numeric values\n",
        "  indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
        "  data = indexer.fit(data).transform(data)\n",
        "\n",
        "  # Create feature vector\n",
        "  assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
        "  data = assembler.transform(data.na.drop())\n",
        "\n",
        "  # Cast the columns to double data type\n",
        "  data = data.withColumn(\"sepal_length\", col(\"sepal_length\").cast(\"double\"))\n",
        "  data = data.withColumn(\"sepal_width\", col(\"sepal_width\").cast(\"double\"))\n",
        "  data = data.withColumn(\"petal_length\", col(\"petal_length\").cast(\"double\"))\n",
        "  data = data.withColumn(\"petal_width\", col(\"petal_width\").cast(\"double\"))\n",
        "\n",
        "  # Split the data into training and test sets\n",
        "  trainData, testData = data.randomSplit([0.7, 0.3], seed=123)\n",
        "\n",
        "  # Train the logistic regression model\n",
        "  lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", family=\"multinomial\", maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
        "  lrModel = lr.fit(trainData)\n",
        "  return lrModel, testData\n",
        "\n",
        "\n",
        "\n",
        "def evaluation():\n",
        "\n",
        "  \"\"\"  \n",
        "  This function takes the input by excuting evaluation and evaluated logistic regression machine learning model\n",
        "  Args: \n",
        "    None\n",
        "  Returns:\n",
        "    This function returns machine learning models accuracy.\n",
        "  \"\"\"\n",
        "\n",
        "  lrModel, testData = lr_model_build()\n",
        "  # Make predictions on test data and evaluate the accuracy\n",
        "  predictions = lrModel.transform(testData)\n",
        "  evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "  accuracy = evaluator.evaluate(predictions)\n",
        "  return accuracy\n",
        "\n",
        "evaluation()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "a077wBA_rEVC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "import csv\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"LogisticRegressionExample\").getOrCreate()\n",
        "\n",
        "\n",
        "def out_3_2(li_val_1=[5.1, 3.5, 1.4, 0.2], li_val_2=[6.2, 3.4, 5.4, 2.3]):\n",
        "    \"\"\"\n",
        "    This function takes the input by executing lr_model_build\n",
        "    Args:\n",
        "      custom value li_val_1 and li_val_2 is used for prediction\n",
        "    Returns:\n",
        "      This function returns predicted results.\n",
        "    \"\"\"\n",
        "    lrModel, testData = lr_model_build()\n",
        "\n",
        "    value1 = Vectors.dense(li_val_1)\n",
        "    value2 = Vectors.dense(li_val_2)\n",
        "\n",
        "    # Create a DataFrame with the custom values\n",
        "    pred_data = spark.createDataFrame([(value1,), (value2,)], [\"features\"])\n",
        "\n",
        "    # Use the trained logistic regression model to make predictions on the custom values\n",
        "    predictions = lrModel.transform(pred_data)\n",
        "\n",
        "    # # Show the predicted class for each custom value\n",
        "    # predictions.select(\"prediction\").show()\n",
        "    prediction_list = [row.prediction for row in predictions.collect()]\n",
        "\n",
        "    return prediction_list\n",
        "\n",
        "\n",
        "def write_out_3_2():\n",
        "    \"\"\"\n",
        "    This function takes the input by executing out_3_2\n",
        "    Args:\n",
        "      None\n",
        "    Returns:\n",
        "      This function saves an output file holding the class names.\n",
        "    \"\"\"\n",
        "    prediction_list = out_3_2()\n",
        "    with open('out_3_2.txt', 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['class'])\n",
        "        for prediction in prediction_list:\n",
        "            if prediction == 0.0:\n",
        "                writer.writerow(['Iris-setosa'])\n",
        "            elif prediction == 1.0:\n",
        "                writer.writerow(['Iris-versicolor'])\n",
        "            elif prediction == 2.0:\n",
        "                writer.writerow(['Iris-virginica'])\n",
        "\n",
        "\n",
        "write_out_3_2()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "mwWon2y4vxvX"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}